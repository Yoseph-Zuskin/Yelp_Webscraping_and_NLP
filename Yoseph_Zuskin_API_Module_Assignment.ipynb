{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Module Project by Yoseph Zuskin, 2019-12-26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the packages needed for this Notebook\n",
    "import requests\n",
    "import json\n",
    "import urllib3\n",
    "import certifi\n",
    "import bs4\n",
    "import time\n",
    "import random\n",
    "from math import ceil\n",
    "from string import punctuation\n",
    "from inflect import engine\n",
    "from collections import defaultdict\n",
    "from datetime import date, datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk.sentiment.util\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.classify import NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Yelp Fusion API to Obtain List of Toronto Businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your Yelp Fusion API key from text file in same folder as this Notebook\n",
    "with open('Yelp_API_Key.txt', 'r') as file:\n",
    "    yelp_key = file.read()\n",
    "    \n",
    "# Alternatively, you may comment out the lines above and uncomment the line below to directly insert your API key\n",
    "#yelp_key = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the string to initiate the Yelp Fusion API requests\n",
    "yelp_fusion_api_key = {'Authorization': 'Bearer %s' % yelp_key}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the Yelp aliases of 1,000 businesses in Toronto by 20 iterations which collect 50 aliases in each,\n",
    "# in order to conform with the limitations of the Yelp Fusion API search capability\n",
    "toronto_businesses = list() # First create an empty list into which the JSON data will be added\n",
    "endpoint = 'https://api.yelp.com/v3/businesses/search?location=toronto' # Declare the API endpoint\n",
    "for i in range(0,1000,50): # Iterate over 0 to 950 to offset the search by that number each iteration\n",
    "    # Submit a get request from the Yelp Fusion API to obtain 50 business aliases in each iteration\n",
    "    r = requests.get(endpoint+'&limit=50&offset='+str(i), # Offset for the next 50 businesses\n",
    "                     # Activate the API using the key which you uploaded via text file in an ealier cell\n",
    "                     headers=yelp_fusion_api_key)\n",
    "    # Append the resulting list of 50 business Yelp alises to the existing list of Toronto businesses\n",
    "    toronto_businesses.extend(json.loads(r.content)['businesses'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate a random seed instance of your choosing (randomly select your own seed if you choose to run this)\n",
    "random.seed(454)\n",
    "\"\"\"\n",
    "The code below will randomly select 120 Toronto businesses from the list obtained from the cell above in order\n",
    "to use their aliases to scrape the Yelp.ca website for full reviews. The webscraping code written in the cells\n",
    "below should take approximately one and a half hours or so to completely loop over 2 Yelp pages for each of the\n",
    "Toronto businesses and obtain as many reviews as possible from those two pages. If you would like to reduce the\n",
    "time it will take the webscraping to complete, reduce the number of Toronto businesses which are randomly sampled\n",
    "by changing the k parameter in the random.sample function called in the line below.\n",
    "\"\"\"\n",
    "randomly_sampled_toronto_businesses=random.sample([business for business in toronto_businesses\n",
    "                                                   if business['review_count']>=40], k=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webscraping Yelp.ca for Full Reviews of a Sample of Toronto Businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate an instance of the urllib3 PoolManager to handle URL calls in the loop below\n",
    "http = urllib3.PoolManager(cert_reqs='CERT_REQUIRED',ca_certs=certifi.where())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initate an instane of an empty list into which the labeled reviews will be added\n",
    "review_labels = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initiate an empty log file into which the webscraping performance will be recorded\n",
    "log_file = open('webscraping_log_'+datetime.today().strftime('%Y-%m-%d_%H-%M-%S')+'.txt', mode='w+')\n",
    "log_file.write(datetime.today().strftime('%m/%d/%Y, %H:%M:%S - ')+'Created webscraper log file.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-f571be3ca6ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;31m# Use the BeautifulSoup package to select the 20 reviews shown on this page and transform the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;31m# into stirng form, and use the JSON package to transform the data to dictionary form\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m     \u001b[0mreveiews_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'script'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m     \u001b[1;31m# Define the list of the next 20 or so review descriptions for this Toronto business\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[0mreviews\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mreview\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'description'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreveiews_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'review'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "seed = 1226 # Choose your own random seed state to help further mask the webscraper by adding randomness\n",
    "random.seed(seed) # Initiate the randon seed state of your choosing (please select one different from mine)\n",
    "reviews_counter = 0 # Initiate a reviews counter to determine how many reviews are webscraped using this loop\n",
    "start_time = datetime.today() # Declare a variable capturing the time when the webscraping began\n",
    "log_file.write(start_time.strftime('%m/%d/%Y, %H:%M:%S - ')+'Began the Yelp reviews webscraping loop with'+\n",
    "               ' a random seed state of '+str(seed)+'\\n') # Record that the webscraping loop is beginning\n",
    "\n",
    "# Iterate over the list of Toronto business alieses to obtain the full content of all its Yelp reviews to date\n",
    "for alias in [toronto_business['alias'] for toronto_business in randomly_sampled_toronto_businesses]:\n",
    "    \n",
    "    \"\"\" visiting the url of the Toronto business containing the first 20 reviews \"\"\"\n",
    "    # Define the URL for the Yelp page of the Toronto business using the selected alias\n",
    "    url = 'https://www.yelp.ca/biz/'+alias\n",
    "    # Use the urllib3 PoolManager instance to go to the Yelp page of the Toronto business\n",
    "    response = http.request('GET', url)\n",
    "    # Record the URL \"visit\" to Toronto business' first Yelp page in the webscraper log file\n",
    "    log_file.write(datetime.today().strftime('%m/%d/%Y, %H:%M:%S - ')+\n",
    "                   'Visited the first page of Yelp reviews for '+alias+'.\\n')\n",
    "    # Use the BeautifulSoup package to obtain the HTML of the website \"visited\" in the code above\n",
    "    html = bs4.BeautifulSoup(response.data)\n",
    "    # Use the BeautifulSoup package to select the 20 reviews shown on this page and transform the data\n",
    "    # into stirng form, and use the JSON package to transform the data to dictionary form\n",
    "    reveiews_data = json.loads(html.find_all('script')[7].text)\n",
    "    # Determine the total number of Yelp reviews which this Toronto business has recieved\n",
    "    reveiws_num = reveiews_data['aggregateRating']['reviewCount']\n",
    "    # As there are 20 reviews per page, determine the number of pages of reviews this Torono business has\n",
    "    pages_num = ceil(reveiws_num / 20)\n",
    "    # Define the list of the first 20 review descriptions for this Toronto business\n",
    "    reviews = [review['description'] for review in reveiews_data['review']]\n",
    "    # Define the list of the first 20 review ratings for this Toronto business\n",
    "    ratings = [rating['reviewRating']['ratingValue'] for rating in reveiews_data['review']]\n",
    "    # Record the number of reviews webscraped from this page in the webscraper log file\n",
    "    log_file.write(datetime.today().strftime('%m/%d/%Y, %H:%M:%S - ')+\n",
    "                   'Successfully webscraped '+str(len(reviews))+' Yelp reviews from the first page.\\n')\n",
    "    reviews_counter += len(reviews) # Update the number of Yelp reviews successfully webscrapped to this point\n",
    "    # Add the first 20 reviews for this Toronto business to the list of review labels\n",
    "    review_labels.extend([[review,rating] for review,rating in zip(reviews,ratings)])\n",
    "    # Pause the loop for 10 to 30 seconds to reduce likelihood of Yelp discovering the scraping\n",
    "    first_pause = random.randint(10, 30) # First randomly select how many seconds this pause will last for\n",
    "    # Record this pause duration in the webscraper log file\n",
    "    log_file.write(datetime.today().strftime('%m/%d/%Y, %H:%M:%S - ')+\n",
    "                   'Randomly decided to pause for '+str(first_pause)+' seconds before visiting next page.\\n')\n",
    "    time.sleep(first_pause) # Then actually pause the code for that timeframe\n",
    "    \n",
    "    \"\"\" visiting a randomly selected next Yelp page containing another 20 or so reviews \"\"\"\n",
    "    # To reduce risk of getting caught by Yelp for scraping, randomly select the next page to scrape\n",
    "    # within either a range of 1 to 8 as that is the options for additional pages given to real page\n",
    "    # visitors, or from 1 to pages_num if there are less than 9 pages of review for this business,\n",
    "    # and there is a chance that the last page will not have 20 reviews so the loop will never go to\n",
    "    # the 9th or last page of this Toronto business's Yelp reviews\n",
    "    next_page = random.randint(1,min(8,pages_num-1))\n",
    "    # Define the URL for the next Yelp page of the Toronto business using the selected alias\n",
    "    url = 'https://www.yelp.ca/biz/'+alias+'?start='+str(next_page*20)\n",
    "    # Use the urllib3 PoolManager instance to go to the Yelp page of the Toronto business\n",
    "    response = http.request('GET', url)\n",
    "    # Record the URL \"visit\" to Toronto business' next selected Yelp page in the webscraper log file\n",
    "    log_file.write(datetime.today().strftime('%m/%d/%Y, %H:%M:%S - ')+\n",
    "                   'Visited page number '+str(next_page)+' of Yelp reviews for '+alias+'\\n')\n",
    "    # Use the BeautifulSoup package to obtain the HTML of the website \"visited\" in the code above\n",
    "    html = bs4.BeautifulSoup(response.data)\n",
    "    # Use the BeautifulSoup package to select the 20 reviews shown on this page and transform the data\n",
    "    # into stirng form, and use the JSON package to transform the data to dictionary form\n",
    "    reveiews_data = json.loads(html.find_all('script')[7].text)\n",
    "    # Define the list of the next 20 or so review descriptions for this Toronto business\n",
    "    reviews = [review['description'] for review in reveiews_data['review']]\n",
    "    # Define the list of the next 20 or so review ratings for this Toronto business\n",
    "    ratings = [rating['reviewRating']['ratingValue'] for rating in reveiews_data['review']]\n",
    "    # Record the number of reviews webscraped from this page in the webscraper log file\n",
    "    log_file.write(datetime.today().strftime('%m/%d/%Y, %H:%M:%S - ')+\n",
    "                   'Successfully webscraped '+str(len(reviews))+' Yelp reviews from page number '+\n",
    "                   str(next_page)+'.\\n')\n",
    "    reviews_counter += len(reviews) # Update the number of Yelp reviews successfully webscrapped to this point\n",
    "    # Add the next 20 or so reviews for this Toronto business to the list of review labels\n",
    "    review_labels.extend([[review,rating] for review,rating in zip(reviews,ratings)])    \n",
    "    # Pause the loop for 10 to 30 seconds to reduce likelihood of Yelp discovering the scraping\n",
    "    second_pause = random.randint(10, 30) # First randomly select how many seconds this pause will last for\n",
    "    # Record this pause duration in the webscraper log file\n",
    "    log_file.write(datetime.today().strftime('%m/%d/%Y, %H:%M:%S - ')+\n",
    "                   'Randomly decided to pause for '+str(second_pause)+' seconds before moving to next business.\\n')\n",
    "    time.sleep(second_pause) # Then actually pause the code for that timeframe\n",
    "end_time = datetime.today() # Declare a variable capturing the time when the webscraping was finished\n",
    "run_time = end_time - start_time # Declare a timedelta variable which states how long the webscraper took to run\n",
    "# Record the successful completion of the loop in the webscaping log file\n",
    "log_file.write(end_time.strftime('%m/%d/%Y, %H:%M:%S - ')+'Completed the Yelp reviews webscraping loop with '+\n",
    "               'total runtime of '+run_time.strftime('%H:%M:%S.%f'),' with a total of '+str(reviews_counter)+\n",
    "               ' successfully scraped from Yelp.ca.\\n')\n",
    "log_file.close() # Close the webscraping log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2899"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of successfully webscraped reviews\n",
    "len(review_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the resulting webscraped Yelp review labels as a a json file\n",
    "with open('data_retrieved_'+datetime.today().strftime('%Y-%m-%d')+'.json','x') as file:\n",
    "    file.write(json.dumps(review_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Deduplicate Yelp Reviews Webscraped on 2019-12-25&26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data of Yelp reviews retrieved on 2019-12-25 as list of lists\n",
    "with open('data_retrieved_2019-12-25.json', 'r') as file:\n",
    "    data_2019_12_25 = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data of Yelp reviews retrieved on 2019-12-25 as list of lists\n",
    "with open('data_retrieved_2019-12-26.json', 'r') as file:\n",
    "    data_2019_12_26 = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the webscraped Yelp reviews data as a single list of lists\n",
    "combined_data = sorted(data_2019_12_25+data_2019_12_26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare a list of review labels by deduplicating the combined datasets of webscraped Yelp reviews\n",
    "review_labels = [combined_data[i] for i in range(len(combined_data)) if i == 0 or\n",
    "                combined_data[i] != combined_data[i-1]]\n",
    "# Or set the review labels to either of the two individual datasets by uncommenting the line below\n",
    "# and commenting out the two lines above this long comment\n",
    "#review_labels = data_2019_12_26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the Yelp Reviews into Tokenized Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function which will remove unwanted characted from the reviews\n",
    "def remove_characters(string,exclude):\n",
    "    \"\"\"\n",
    "    Remove Unwanted Characters from String:\n",
    "    This function removes specified characters which one wishes to exclude from a string\n",
    "    \n",
    "    Parameters:\n",
    "    string (str): The string which will be filtered to exclude unwanted characters\n",
    "    exclude (list of str): The list of unwanted characters which are to be excluded\n",
    "    \n",
    "    Returns:\n",
    "    string (str): Returns the string without the unwanted characters\n",
    "    \"\"\"\n",
    "    for character in exclude: # Loop over all cha\n",
    "        if character == ('\\t' or '\\n' or '\\r' or '\\x0b' or '\\x0c'):\n",
    "            string = string.replace(character,' ')\n",
    "        elif character == '  ':\n",
    "            string = string.replace(character,' ')\n",
    "        else:\n",
    "            string = string.replace(character,'')\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of characters to be removed from the review descriptions\n",
    "exclude_characters = ['\\t','\\n','\\r','\\x0b','\\x0c','  ']+list(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unwanted characters from review descriptions and transform the rating into either positive or negative\n",
    "review_features = [[remove_characters(x,exclude_characters).split(' '),\n",
    "                   'positive' if y > 3 else 'negative'] for (x, y) in review_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\zuski\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Use the nltk package to obtain a list of stop words to be excluded from the NLP\n",
    "nltk.download('stopwords') # First download the if not done so already\n",
    "stop_words = nltk.corpus.stopwords.words('english') # Save list of stop word\n",
    "# Also include variants of the stop words which exclude apostrophe marks\n",
    "stop_words = stop_words + [word.replace(\"'\",\"\") for word in stop_words if \"'\" in word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['don',\n",
       " \"don't\",\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'youre',\n",
       " 'youve',\n",
       " 'youll',\n",
       " 'youd',\n",
       " 'shes',\n",
       " 'its',\n",
       " 'thatll',\n",
       " 'dont',\n",
       " 'shouldve',\n",
       " 'arent',\n",
       " 'couldnt',\n",
       " 'didnt',\n",
       " 'doesnt',\n",
       " 'hadnt',\n",
       " 'hasnt',\n",
       " 'havent',\n",
       " 'isnt',\n",
       " 'mightnt',\n",
       " 'mustnt',\n",
       " 'neednt',\n",
       " 'shant',\n",
       " 'shouldnt',\n",
       " 'wasnt',\n",
       " 'werent',\n",
       " 'wont',\n",
       " 'wouldnt']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# based on the following Medium post, decided to not exclude stopwords which might hint at a negative sentiment:\n",
    "# https://towardsdatascience.com/why-you-should-avoid-removing-stopwords-aa7a353d2a52\n",
    "stop_words[131:133]+stop_words[143:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust the list of stop_words to exclude the ones shown above\n",
    "stop_words = stop_words[:131]+stop_words[133:143]\n",
    "# however, it seems to have made no impact on the classifier model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the stop words from all the reviews and set all tokenized words to lowercase\n",
    "for review in review_features: # Loop over each review that was scraped from Yelp.ca\n",
    "    tokens = review[0] # Select the unfiltered word tokens from this review\n",
    "    # Then filter the word tokens to exclude the stop words declared above\n",
    "    tokens = [word.lower() for word in tokens if word.lower() not in stop_words]\n",
    "    review[0] = tokens # Replace the unfilitered list of word tokens with the filtered tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out empty word tokens from the review features\n",
    "for review in review_features: # Loop over each review that was scraped from Yelp.ca\n",
    "    tokens = review[0] # Select the unfiltered word tokens from this review\n",
    "    while '' in tokens: # Loop over all empty strings\n",
    "        tokens.remove('') # Remove each empty string\n",
    "    review[0] = tokens # Replace filtered tokens with the same list but without empty strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After running the classification model once without using this function to transform the strings\n",
    "# of numbers into phrases describing the numbers, and then once with this processing applied to the\n",
    "# reviews features, it appears it did not improve the model and in fact made it slightly worse, so\n",
    "# in practice it might be better to not do this preprocessing and generally leave the tokens less\n",
    "# processed than the extent to which they were processed in this Notebook\n",
    "def check_if_num(string):\n",
    "    \"\"\"\n",
    "    Check If String is a Number:    \n",
    "    This function checks if the string is actually a number by trying to convert it to a float\n",
    "    \n",
    "    Parameters:\n",
    "    string (str): String which is to be checked\n",
    "    \n",
    "    Returns:\n",
    "    bool: Whether it is true or false that the string is a number\n",
    "    \"\"\"\n",
    "    try: # Tries to convert the string to a float\n",
    "        float(string) # By applying the float funtion\n",
    "    except: # If the conversion to a float is unsuccessful\n",
    "        return False # Returns False if the string cannot be converted to a float\n",
    "    else: # If the conversion to a float is successful\n",
    "        return True # Returns True if the string cannot be converted to a float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform strings of numbers into the written out words describing that number\n",
    "for review in review_features: # Loop over each review that was scraped from Yelp.ca\n",
    "    tokens = review[0] # Select the unfiltered word tokens from this review\n",
    "    for i in range(len(tokens)): # Loop over all tokenized words\n",
    "        if check_if_num(tokens[i]): # Check if the word happnes to be a string of a number\n",
    "            tokens[i] = engine().number_to_words(tokens[i])\n",
    "            if ' ' in tokens[i]: # Check if the resulting string is a long phrase delimited by spaces\n",
    "                tokens[i] = tokens[i].replace(' ','-') # Replace spaces with hyphens if that's the case\n",
    "        else: # Otherwise skip this tokenized word as no further processing is needed\n",
    "            continue # Continue to next word if not a number\n",
    "    review[0] = tokens # Replace the unfilitered list of word tokens with the filtered tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6614"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the number of reviews which were successfully webscraped and tokenized from Yelp.ca\n",
    "len(review_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Number of Reviews')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ratings_distribution = defaultdict(int) # Define an instance of a defaultdict into which the data will be populated\n",
    "for rating in review_labels: # Loop over all ratings in the review labels, which the defaultdict will treat as int\n",
    "    ratings_distribution[rating[1]] += 1 # Raise the count for that rating in the defaultdict based on the review\n",
    "# Then visualize the distribution of review ratings using a barplot\n",
    "plt.bar(ratings_distribution.keys(), ratings_distribution.values(), color='green')\n",
    "plt.title('Distribution of Ratings across Webscraped Yelp Reviews')\n",
    "plt.xlabel('Review Ratings')\n",
    "plt.ylabel('Number of Reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "280"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check number of one start reviews\n",
    "ratings_distribution[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-0a41b64623f5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mreview_features\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "review_features[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat the processed tokenized reviews as a list of tuples after the processing has been completed\n",
    "review_features = [(review,rating) for review,rating in review_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Review Features for a Simple Naive Bayes Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set: 4630, Testing Set: 1984\n"
     ]
    }
   ],
   "source": [
    "# Shuffle the reviews and separate 70% into a training and 30% into test set\n",
    "random.seed(32) # First pick a random seed stage value to randomize the shuffle\n",
    "random.shuffle(review_features) # Then apply the random shuffling to the reviews\n",
    "gap = round(len(review_features)*0.7)\n",
    "training_set = review_features[:gap] # Separate into training set of reviews\n",
    "test_set = review_features[gap:] # Separate into the test set of reviews\n",
    "# Print the number of reviews in the training and test sets based on the split\n",
    "print(\"Training Set: %d, Testing Set: %d\" % (len(training_set), len(test_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate an instance of the nltk SentimentAnalyzer\n",
    "sentiment = SentimentAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of negation words using the mark_negation class of the nltk sentiment utility subpackage\n",
    "all_words_neg = sentiment.all_words([nltk.sentiment.util.mark_negation(review[0], double_neg_flip = True) # <-,\n",
    "                                     for review in training_set]) # Enables recognization of double negations/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the feature extraction method for the nltk SentimerAnalyzer class\n",
    "unigram_feats = sentiment.unigram_word_feats(words=all_words_neg, min_freq=4)\n",
    "sentiment.add_feat_extractor(nltk.sentiment.util.extract_unigram_feats, unigrams=unigram_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the feature extraction method defined above on the training and test sets\n",
    "training_set_features = sentiment.apply_features(training_set)\n",
    "test_set_features = sentiment.apply_features(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier\n",
      "Evaluating NaiveBayesClassifier results...\n",
      "Accuracy: 0.8039314516129032\n",
      "F-measure [negative]: 0.5900948366701791\n",
      "F-measure [positive]: 0.8711493872143095\n",
      "Precision [negative]: 0.6320541760722348\n",
      "Precision [positive]: 0.8533419857235561\n",
      "Recall [negative]: 0.5533596837944664\n",
      "Recall [positive]: 0.8897158322056834\n"
     ]
    }
   ],
   "source": [
    "trainer = NaiveBayesClassifier.train # Define instance of the nltk Naive Bayes Classifier class\n",
    "classifier = sentiment.train(trainer, training_set_features) # Train classifier on training set features\n",
    "for key,value in sorted(sentiment.evaluate(test_set_features).items()): # Loop over results of the clasifier model\n",
    "     print('{0}: {1}'.format(key, value)) # Evaluate the results of the Naive Bayes Classifier model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
